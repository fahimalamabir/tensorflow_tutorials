{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives**\n",
    "\n",
    "1. Understand Basic and Advanced Tensor Concepts\n",
    "2. Understand Single-Axis and Multi-Axis Indexing\n",
    "3. Create Tensors and Variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Introduction \n",
    "\n",
    "In this notebook, we look at tensors, which are multi-dimensional arrays with a uniform type (called a dtype). You can see all supported dtypes at [tf.dtypes.DType](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType).  If you're familiar with [ NumPy](https://numpy.org/devdocs/user/quickstart.html), tensors are (kind of) like np.arrays.  All tensors are immutable like python numbers and strings: you can never update the contents of a tensor, only create a new one.\n",
    "\n",
    "We also look at variables, a `tf.Variable` represents a tensor whose value can be changed by running operations (ops) on it.  Specific ops allow you to read and modify the values of this tensor. Higher level libraries like `tf.keras` use `tf.Variable` to store model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a \"scalar\" or \"rank-0\" tensor . A scalar contains a single value, and no \"axes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant(3) #scalar\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"vector\" or \"rank-1\" tensor is like a list of values. A vector has 1-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 2], dtype=int32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([3,2]) #vector\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"matrix\" or \"rank-2\" tensor has 2-axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 4]], dtype=int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[1,2],[3,4]]) #matrix\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "  <th>A matrix, shape: <code>[3, 2]</code></th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"matrix.png\" alt=\"A 3x2 grid, with each cell containing a number.\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]], shape=(3, 2), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "# If we want to be specific, we can set the dtype (see below) at creation time\n",
    "# TODO 1a\n",
    "rank_2_tensor = tf.constant([[1, 2],\n",
    "                             [3, 4],\n",
    "                             [5, 6]], dtype=tf.float16)\n",
    "print(rank_2_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways you might visualize a tensor with more than 2-axes.\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=3>A 3-axis tensor, shape: <code>[3, 2, 5]</code></th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"3-axis_numpy.png\"/>\n",
    "  </td>\n",
    "  <td>\n",
    "   <img src=\"3-axis_front.png\"/>\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "   <img src=\"3-axis_block.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 2), dtype=int32, numpy=\n",
       "array([[[1, 2],\n",
       "        [2, 3]],\n",
       "\n",
       "       [[2, 4],\n",
       "        [5, 6]],\n",
       "\n",
       "       [[7, 8],\n",
       "        [1, 7]]], dtype=int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[[1,2],[2,3]],\n",
    "                [[2,4],[5,6]],\n",
    "                [[7,8],[1,7]]]) #3D tensor\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a tensor to a NumPy array either using `np.array` or the `tensor.numpy` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.],\n",
       "       [5., 6.]], dtype=float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a tensor to a NumPy array using `np.array` method\n",
    "# TODO 1b\n",
    "np.array(rank_2_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.],\n",
       "       [5., 6.]], dtype=float16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a tensor to a NumPy array using `tensor.numpy` method\n",
    "# TODO 1c\n",
    "rank_2_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors often contain floats and ints, but have many other types, including:\n",
    "\n",
    "* complex numbers\n",
    "* strings\n",
    "\n",
    "The base `tf.Tensor` class requires tensors to be \"rectangular\"---that is, along each axis, every element is the same size.  However, there are specialized types of Tensors that can handle different shapes: \n",
    "\n",
    "* ragged\n",
    "* sparse\n",
    "\n",
    "We can do basic math on tensors, including addition, element-wise multiplication, and matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[3 3]\n",
      " [7 7]], shape=(2, 2), dtype=int32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "b = tf.constant([[1, 1],\n",
    "                 [1, 1]]) # Could have also said `tf.ones([2,2])`\n",
    "\n",
    "print(tf.add(a, b), \"\\n\")\n",
    "print(tf.multiply(a, b), \"\\n\")\n",
    "print(tf.matmul(a, b), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[3 3]\n",
      " [7 7]], shape=(2, 2), dtype=int32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a + b, \"\\n\") # element-wise addition\n",
    "print(a * b, \"\\n\") # element-wise multiplication\n",
    "print(a @ b, \"\\n\") # matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are used in all kinds of operations (ops).We can do the same thing by stack method of Tf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2, 3), dtype=int32, numpy=\n",
       "array([[[1, 3, 4],\n",
       "        [1, 3, 4]],\n",
       "\n",
       "       [[1, 3, 4],\n",
       "        [1, 3, 4]],\n",
       "\n",
       "       [[1, 3, 4],\n",
       "        [1, 3, 4]],\n",
       "\n",
       "       [[1, 3, 4],\n",
       "        [1, 3, 4]]], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1,3,4]) #(3,)\n",
    "a1 = tf.stack([a,a]) #(2,3)\n",
    "a2 = tf.stack([a1, a1, a1, a1]) #(4,2,3)\n",
    "a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way, we stacked up the matrices, we could slice them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]], dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = tf.constant([[1,2,3],[4,5,6]])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "say, you are asked to get (2,5), THEN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]], dtype=int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.reshape(s, [3,2]) # get 3X2 matrix\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.0, shape=(), dtype=float32)\n",
      "tf.Tensor([1 0], shape=(2,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[2.6894143e-01 7.3105860e-01]\n",
      " [9.9987662e-01 1.2339458e-04]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c = tf.constant([[4.0, 5.0], [10.0, 1.0]])\n",
    "\n",
    "# Find the largest value\n",
    "print(tf.reduce_max(c))\n",
    "# Find the index of the largest value\n",
    "print(tf.argmax(c))\n",
    "# Compute the softmax\n",
    "print(tf.nn.softmax(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About shapes\n",
    "\n",
    "Tensors have shapes.  Some vocabulary:\n",
    "\n",
    "* **Shape**: The length (number of elements) of each of the dimensions of a tensor.\n",
    "* **Rank**: Number of tensor dimensions.  A scalar has rank 0, a vector has rank 1, a matrix is rank 2.\n",
    "* **Axis** or **Dimension**: A particular dimension of a tensor.\n",
    "* **Size**: The total number of items in the tensor, the product shape vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_4_tensor = tf.zeros([3, 2, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "  <th colspan=2>A rank-4 tensor, shape: <code>[3, 2, 4, 5]</code></th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "<img src=\"shape.png\" alt=\"A tensor shape is like a vector.\">\n",
    "    <td>\n",
    "<img src=\"4-axis_block.png\" alt=\"A 4-axis tensor\">\n",
    "  </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of every element: <dtype: 'float32'>\n",
      "Number of dimensions: 4\n",
      "Shape of tensor: (3, 2, 4, 5)\n",
      "Elements along axis 0 of tensor: 3\n",
      "Elements along the last axis of tensor: 5\n",
      "Total number of elements (3*2*4*5):  120\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of every element:\", rank_4_tensor.dtype)\n",
    "print(\"Number of dimensions:\", rank_4_tensor.ndim)\n",
    "print(\"Shape of tensor:\", rank_4_tensor.shape)\n",
    "print(\"Elements along axis 0 of tensor:\", rank_4_tensor.shape[0])\n",
    "print(\"Elements along the last axis of tensor:\", rank_4_tensor.shape[-1])\n",
    "print(\"Total number of elements (3*2*4*5): \", tf.size(rank_4_tensor).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Typical axis order</th>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "<img src=\"shape2.png\" alt=\"Keep track of what each axis is. A 4-axis tensor might be: Batch, Width, Height, Freatures\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand Single-Axis and Multi-Axis Indexing\n",
    "\n",
    "### Single-axis indexing\n",
    "\n",
    "TensorFlow follow standard python indexing rules, similar to [indexing a list or a string in python](https://docs.python.org/3/tutorial/introduction.html#strings), and the bacic rules for numpy indexing.\n",
    "\n",
    "* indexes start at `0`\n",
    "* negative indices count backwards from the end\n",
    "* colons, `:`, are used for slices `start:stop:step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  2  3  5  8 13 21 34]\n"
     ]
    }
   ],
   "source": [
    "rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])\n",
    "print(rank_1_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing with a scalar removes the dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First: 0\n",
      "Second: 1\n",
      "Last: 34\n"
     ]
    }
   ],
   "source": [
    "print(\"First:\", rank_1_tensor[0].numpy())\n",
    "print(\"Second:\", rank_1_tensor[1].numpy())\n",
    "print(\"Last:\", rank_1_tensor[-1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing with a `:` slice keeps the dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything: [ 0  1  1  2  3  5  8 13 21 34]\n",
      "Before 4: [0 1 1 2]\n",
      "From 4 to the end: [ 3  5  8 13 21 34]\n",
      "From 2, before 7: [1 2 3 5 8]\n",
      "Every other item: [ 0  1  3  8 21]\n",
      "Reversed: [34 21 13  8  5  3  2  1  1  0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Everything:\", rank_1_tensor[:].numpy())\n",
    "print(\"Before 4:\", rank_1_tensor[:4].numpy())\n",
    "print(\"From 4 to the end:\", rank_1_tensor[4:].numpy())\n",
    "print(\"From 2, before 7:\", rank_1_tensor[2:7].numpy())\n",
    "print(\"Every other item:\", rank_1_tensor[::2].numpy())\n",
    "print(\"Reversed:\", rank_1_tensor[::-1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-axis indexing\n",
    "\n",
    "Higher rank tensors are indexed by passing multiple indices. \n",
    "\n",
    "The single-axis exact same rules as in  the single-axis case apply to each axis independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "print(rank_2_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing an integer for each index the result is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# Pull out a single value from a 2-rank tensor\n",
    "print(rank_2_tensor[1, 1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can index using any combination integers and slices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second row: [3. 4.]\n",
      "Second column: [2. 4. 6.]\n",
      "Last row: [5. 6.]\n",
      "First item in last column: 2.0\n",
      "Skip the first row:\n",
      "[[3. 4.]\n",
      " [5. 6.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get row and column tensors\n",
    "print(\"Second row:\", rank_2_tensor[1, :].numpy())\n",
    "print(\"Second column:\", rank_2_tensor[:, 1].numpy())\n",
    "print(\"Last row:\", rank_2_tensor[-1, :].numpy())\n",
    "print(\"First item in last column:\", rank_2_tensor[0, -1].numpy())\n",
    "print(\"Skip the first row:\")\n",
    "print(rank_2_tensor[1:, :].numpy(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example with a 3-axis tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 4  9]\n",
      " [14 19]\n",
      " [24 29]], shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "rank_3_tensor = tf.constant([\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9]],\n",
    "  [[10, 11, 12, 13, 14],\n",
    "   [15, 16, 17, 18, 19]],\n",
    "  [[20, 21, 22, 23, 24],\n",
    "   [25, 26, 27, 28, 29]],])\n",
    "print(rank_3_tensor[:, :, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th colspan=2>Selecting the last feature across all locations in each example in the batch </th>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "<img src=\"index1.png\" alt=\"A 3x2x5 tensor with all the values at the index-4 of the last axis selected.\">\n",
    "  </td>\n",
    "      <td>\n",
    "<img src=\"index2.png\" alt=\"The selected values packed into a 2-axis tensor.\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "## Manipulating Shapes\n",
    "\n",
    "Reshaping a tensor is of great utility. \n",
    "\n",
    "The `tf.reshape` operation is fast and cheap as the underlying data does not need to be duplicated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Shape returns a `TensorShape` object that shows the size on each dimension\n",
    "var_x = tf.Variable(tf.constant([[1], [2], [3]]))\n",
    "print(var_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1]\n"
     ]
    }
   ],
   "source": [
    "# You can convert this object into a Python list, too\n",
    "print(var_x.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reshape a tensor into a new shape.  Reshaping is fast and cheap as the underlying data does not need to be duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can reshape a tensor to a new shape.\n",
    "# Note that we're passing in a list\n",
    "reshaped = tf.reshape(var_x, [1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "print(var_x.shape)\n",
    "print(reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data maintains it's layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style \"row-major\" memory ordering, where incrementing the right-most index corresponds to a single step in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(rank_3_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you flatten a tensor you can see what order it is laid out in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29], shape=(30,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# A `-1` passed in the `shape` argument says \"Whatever fits\".\n",
    "print(tf.reshape(rank_3_tensor, [-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically the only reasonable uses of tf.reshape are to combine or split adjacent axes (or add/remove 1s).\n",
    "\n",
    "For this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]\n",
      " [25 26 27 28 29]], shape=(6, 5), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]\n",
      " [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reshape(rank_3_tensor, [3*2, 5]), \"\\n\")\n",
    "print(tf.reshape(rank_3_tensor, [3, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th colspan=3>\n",
    "Some good reshapes.\n",
    "</th>\n",
    "<tr>\n",
    "  <td>\n",
    "<img src=\"reshape-before.png\" alt=\"A 3x2x5 tensor\">\n",
    "  </td>\n",
    "  <td>\n",
    "  <img src=\"reshape-good1.png\" alt=\"The same data reshaped to (3x2)x5\">\n",
    "  </td>\n",
    "  <td>\n",
    "<img src=\"reshape-good2.png\" alt=\"The same data reshaped to 3x(2x5)\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Reshaping will \"work\" for any new shape with the same total number of elements, but it will not do anything useful if you do not respect the order of the axes.\n",
    "\n",
    "Swapping axes in `tf.reshape` does not work, you need `tf.transpose` for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]\n",
      "  [10 11 12 13 14]]\n",
      "\n",
      " [[15 16 17 18 19]\n",
      "  [20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[ 0  1  2  3  4  5]\n",
      " [ 6  7  8  9 10 11]\n",
      " [12 13 14 15 16 17]\n",
      " [18 19 20 21 22 23]\n",
      " [24 25 26 27 28 29]], shape=(5, 6), dtype=int32) \n",
      "\n",
      "Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]\n"
     ]
    }
   ],
   "source": [
    "# Bad examples: don't do this\n",
    "\n",
    "# You can't reorder axes with reshape.\n",
    "print(tf.reshape(rank_3_tensor, [2, 3, 5]), \"\\n\") \n",
    "\n",
    "# This is a mess\n",
    "print(tf.reshape(rank_3_tensor, [5, 6]), \"\\n\")\n",
    "\n",
    "# This doesn't work at all\n",
    "try:\n",
    "  tf.reshape(rank_3_tensor, [7, -1])\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th colspan=3>\n",
    "Some bad reshapes.\n",
    "</th>\n",
    "<tr>\n",
    "  <td>\n",
    "<img src=\"reshape-bad.png\" alt=\"You can't reorder axes, use tf.transpose for that\">\n",
    "  </td>\n",
    "  <td>\n",
    "<img src=\"reshape-bad4.png\" alt=\"Anything that mixes the slices of data together is probably wrong.\">\n",
    "  </td>\n",
    "  <td>\n",
    "<img src=\"reshape-bad2.png\" alt=\"The new shape must fit exactly.\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "You may run across not-fully-specified shapes. Either the shape contains a `None` (a dimension's length is unknown) or the shape is `None` (the rank of the tensor is unknown).\n",
    "\n",
    "Except for [tf.RaggedTensor], this will only occur in the context of TensorFlow's, symbolic, graph-building  APIs: \n",
    "\n",
    "* [tf.function]\n",
    "* The [keras functional API].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on `DTypes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 3 4], shape=(3,), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "# `tf.cast` casts a tensor to a new type.\n",
    "# TODO 2b\n",
    "the_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64)\n",
    "the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16)\n",
    "# Now, let's cast to an uint8 and lose the decimal precision\n",
    "the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8)\n",
    "print(the_u8_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "Broadcasting is a concept borrowed from the [equivalent feature in NumPy].  In short, under certain conditions, smaller tensors are \"stretched\" automatically to fit larger tensors when running combined operations on them.\n",
    "\n",
    "The simplest and most common case is when you attempt to multiply or add a tensor to a scalar.  In that case, the scalar is broadcast to be the same shape as the other argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n",
      "tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n",
      "tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 2, 3])\n",
    "\n",
    "y = tf.constant(2)\n",
    "z = tf.constant([2, 2, 2])\n",
    "# All of these are the same computation\n",
    "print(tf.multiply(x, 2))\n",
    "print(x * y)\n",
    "print(x * z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, 1-sized dimensions can be stretched out to match the other arguments.  Both arguments can be stretched in the same computation.\n",
    "\n",
    "In this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to produce a 3x4 matrix. Note how the leading 1 is optional: The shape of y is `[4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [2]\n",
      " [3]], shape=(3, 1), dtype=int32) \n",
      "\n",
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[ 1  2  3  4]\n",
      " [ 2  4  6  8]\n",
      " [ 3  6  9 12]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# These are the same computations\n",
    "x = tf.reshape(x,[3,1])\n",
    "y = tf.range(1, 5)\n",
    "print(x, \"\\n\")\n",
    "print(y, \"\\n\")\n",
    "print(tf.multiply(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "  <th>A broadcasted add: a <code>[3, 1]</code> times a <code>[1, 4]</code> gives a <code>[3,4]</code> </th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "<img src=\"broadcasting.png\" alt=\"Adding a 3x1 matrix to a 4x1 matrix results in a 3x4 matrix\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Here is the same operation without broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1  2  3  4]\n",
      " [ 2  4  6  8]\n",
      " [ 3  6  9 12]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x_stretch = tf.constant([[1, 1, 1, 1],\n",
    "                         [2, 2, 2, 2],\n",
    "                         [3, 3, 3, 3]])\n",
    "\n",
    "y_stretch = tf.constant([[1, 2, 3, 4],\n",
    "                         [1, 2, 3, 4],\n",
    "                         [1, 2, 3, 4]])\n",
    "\n",
    "print(x_stretch * y_stretch)  # Again, operator overloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory.  \n",
    "\n",
    "You see what broadcasting looks like using `tf.broadcast_to`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [1 2 3]\n",
      " [1 2 3]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike a mathematical op, for example, `broadcast_to` does nothing special to save memory.  Here, you are materializing the tensor.\n",
    "\n",
    "It can get even more complicated.  [This section](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html) of Jake VanderPlas's book _Python Data Science Handbook_ shows more broadcasting tricks (again in NumPy).\n",
    "\n",
    "## tf.convert_to_tensor\n",
    "\n",
    "Most ops, like `tf.matmul` and `tf.reshape` take arguments of class `tf.Tensor`.  However, you'll notice in the above case, we frequently pass Python objects shaped like tensors.\n",
    "\n",
    "Most, but not all, ops call `convert_to_tensor` on non-tensor arguments.  There is a registry of conversions, and most object classes like NumPy's `ndarray`, `TensorShape`, Python lists, and `tf.Variable` will all convert automatically.\n",
    "\n",
    "See `tf.register_tensor_conversion_function` for more details, and if you have your own type you'd like to automatically convert to a tensor.\n",
    "\n",
    "\n",
    "## Ragged Tensors\n",
    "\n",
    "A tensor with variable numbers of elements along some axis is called \"ragged\". Use `tf.ragged.RaggedTensor` for ragged data.\n",
    "\n",
    "For example, This cannot be represented as a regular tensor:\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th>A `tf.RaggedTensor`, shape: <code>[4, None]</code></th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "<img src=\"ragged.png\" alt=\"A 2-axis ragged tensor, each row can have a different length.\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragged_list = [\n",
    "    [0, 1, 2, 3],\n",
    "    [4, 5],\n",
    "    [6, 7, 8],\n",
    "    [9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't convert non-rectangular Python sequence to Tensor.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  tensor = tf.constant(ragged_list)\n",
    "except Exception as e: print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead create a `tf.RaggedTensor` using `tf.ragged.constant`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>\n"
     ]
    }
   ],
   "source": [
    "# `tf.ragged.constant` constructs a constant RaggedTensor from a nested Python list.\n",
    "ragged_tensor = tf.ragged.constant(ragged_list)\n",
    "print(ragged_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of a `tf.RaggedTensor` contains unknown dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, None)\n"
     ]
    }
   ],
   "source": [
    "print(ragged_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String tensors\n",
    "\n",
    "`tf.string` is a `dtype`, which is to say we can represent data as strings (variable-length byte arrays) in tensors.  \n",
    "\n",
    "The strings are atomic and cannot be indexed the way Python strings are. The length of the string is not one of the dimensions of the tensor. See `tf.strings` for functions to manipulate them.\n",
    "\n",
    "Here is a scalar string tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Gray wolf', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Tensors can be strings, too here is a scalar string.\n",
    "scalar_string_tensor = tf.constant(\"Gray wolf\")\n",
    "print(scalar_string_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "  <th>A vector of strings, shape: <code>[3,]</code></th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "<img src=\"strings.png\" alt=\"The string length is not one of the tensor's axes.\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'Gray wolf' b'Quick brown fox' b'Lazy dog'], shape=(3,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# If we have two string tensors of different lengths, this is OK.\n",
    "tensor_of_strings = tf.constant([\"Gray wolf\",\n",
    "                                 \"Quick brown fox\",\n",
    "                                 \"Lazy dog\"])\n",
    "# Note that the shape is (2,), indicating that it is 2 x unknown.\n",
    "print(tensor_of_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above printout the `b` prefix indicates that `tf.string` dtype is not a unicode string, but a byte-string. See the [Unicode Tutorial](https://www.tensorflow.org/tutorials/load_data/unicode) for more about working with unicode text in TensorFlow.\n",
    "\n",
    "If you pass unicode characters they are utf-8 encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'\\xf0\\x9f\\xa5\\xb3\\xf0\\x9f\\x91\\x8d'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(\"ü•≥üëç\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic functions with strings can be found in `tf.strings`, including `tf.strings.split`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable is a tensor whose value can be changed. tf.Variable will typically hold model weights that need to be updated in a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'Gray' b'wolf'], shape=(2,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# We can use split to split a string into a set of tensors\n",
    "print(tf.strings.split(scalar_string_tensor, sep=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'Gray', b'wolf'], [b'Quick', b'brown', b'fox'], [b'Lazy', b'dog']]>\n"
     ]
    }
   ],
   "source": [
    "# ...but it turns into a `RaggedTensor` if we split up a tensor of strings,\n",
    "# as each string might be split into a different number of parts.\n",
    "print(tf.strings.split(tensor_of_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "  <th>Three strings split, shape: <code>[3, None]</code></th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "<img src=\"string-split.png\" alt=\"Splitting multiple strings returns a tf.RaggedTensor\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "And `tf.string.to_number`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  1.  10. 100.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "text = tf.constant(\"1 10 100\")\n",
    "print(tf.strings.to_number(tf.strings.split(text, \" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although you can't use `tf.cast` to turn a string tensor into numbers, you can convert it into bytes, and then into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte strings: tf.Tensor([b'D' b'u' b'c' b'k'], shape=(4,), dtype=string)\n",
      "Bytes: tf.Tensor([ 68 117  99 107], shape=(4,), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "byte_strings = tf.strings.bytes_split(tf.constant(\"Duck\"))\n",
    "byte_ints = tf.io.decode_raw(tf.constant(\"Duck\"), tf.uint8)\n",
    "print(\"Byte strings:\", byte_strings)\n",
    "print(\"Bytes:\", byte_ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse tensors\n",
    "\n",
    "Sometimes, your data is sparse, like a very wide embedding space.  TensorFlow supports `tf.sparse.SparseTensor` and related operations to store sparse data efficiently.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th>A `tf.SparseTensor`, shape: <code>[3, 4]</code></th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "<img src=\"sparse.png\" alt=\"An 3x4 grid, with values in only two of the cells.\">\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)) \n",
      "\n",
      "tf.Tensor(\n",
      "[[1 0 0 0]\n",
      " [0 0 2 0]\n",
      " [0 0 0 0]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Sparse tensors store values by index in a memory-efficient manner\n",
    "sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],\n",
    "                                       values=[1, 2],\n",
    "                                       dense_shape=[3, 4])\n",
    "print(sparse_tensor, \"\\n\")\n",
    "\n",
    "# We can convert sparse tensors to dense\n",
    "print(tf.sparse.to_dense(sparse_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Variables\n",
    "\n",
    "A TensorFlow **variable** is the recommended way to represent shared, persistent state your program manipulates. This guide covers how to create, update, and manage instances of `tf.Variable` in TensorFlow.\n",
    "\n",
    "Variables are created and tracked via the `tf.Variable` class. A `tf.Variable` represents a tensor whose value can be changed by running ops on it.  Specific ops allow you to read and modify the values of this tensor. Higher level libraries like `tf.keras` use `tf.Variable` to store model parameters. \n",
    "\n",
    "\n",
    "## Create a variable\n",
    "\n",
    "To create a variable, provide an initial value.  The `tf.Variable` will have the same `dtype` as the initialization value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set an initial value\n",
    "my_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "# `tf.Variable` will have the same as `dtype`\n",
    "my_variable = tf.Variable(my_tensor)\n",
    "\n",
    "# Variables can be all kinds of types, just like tensors\n",
    "bool_variable = tf.Variable([False, False, False, True])\n",
    "complex_variable = tf.Variable([5 + 4j, 6 + 1j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable looks and acts like a tensor, and, in fact, is a data structure backed by a `tf.Tensor`.  Like tensors, they have a `dtype` and a shape, and can be exported to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (2, 2)\n",
      "DType:  <dtype: 'float32'>\n",
      "As NumPy:  <bound method BaseResourceVariable.numpy of <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[1., 2.],\n",
      "       [3., 4.]], dtype=float32)>>\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape: \",my_variable.shape)\n",
    "print(\"DType: \",my_variable.dtype)\n",
    "print(\"As NumPy: \", my_variable.numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most tensor operations work on variables as expected, although variables cannot be reshaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A variable: <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[1., 2.],\n",
      "       [3., 4.]], dtype=float32)>\n",
      "\n",
      "Viewed as a tensor: tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "Index of highest value: tf.Tensor([1 1], shape=(2,), dtype=int64)\n",
      "\n",
      "Copying and reshaping:  tf.Tensor([[1. 2. 3. 4.]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"A variable:\",my_variable)\n",
    "print(\"\\nViewed as a tensor:\", tf.convert_to_tensor(my_variable))\n",
    "print(\"\\nIndex of highest value:\", tf.argmax(my_variable))\n",
    "\n",
    "# This creates a new tensor; it does not reshape the variable.\n",
    "print(\"\\nCopying and reshaping: \", tf.reshape(my_variable, ([1,4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, variables are backed by tensors. You can reassign the tensor using `tf.Variable.assign`.  Calling `assign` does not (usually) allocate a new tensor; instead, the existing tensor's memory is reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes (2,) and (3,) are incompatible\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable([2.0, 3.0])\n",
    "# This will keep the same dtype, float32\n",
    "a.assign([1, 2]) \n",
    "# Not allowed as it resizes the variable: \n",
    "try:\n",
    "  a.assign([1.0, 2.0, 3.0])\n",
    "except Exception as e: print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use a variable like a tensor in operations, you will usually operate on the backing tensor.  \n",
    "\n",
    "Creating new variables from existing variables duplicates the backing tensors. Two variables will not share the same memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 6.]\n",
      "[2. 3.]\n",
      "[7. 9.]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable([2.0, 3.0])\n",
    "# Create b based on the value of a\n",
    "b = tf.Variable(a)\n",
    "a.assign([5, 6])\n",
    "\n",
    "# a and b are different\n",
    "print(a.numpy())\n",
    "print(b.numpy())\n",
    "\n",
    "# There are other versions of assign\n",
    "print(a.assign_add([2,3]).numpy())  # [7. 9.]\n",
    "print(a.assign_sub([7,9]).numpy())  # [0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifecycles, naming, and watching\n",
    "\n",
    "In Python-based TensorFlow, `tf.Variable` instance have the same lifecycle as other Python objects. When there are no references to a variable it is automatically deallocated.\n",
    "\n",
    "Variables can also be named which can help you track and debug them.  You can give two variables the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[False False]\n",
      " [False False]], shape=(2, 2), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# Create a and b; they have the same value but are backed by different tensors.\n",
    "a = tf.Variable(my_tensor, name=\"Mark\")\n",
    "# A new variable with the same name, but different value\n",
    "# Note that the scalar add is broadcast\n",
    "b = tf.Variable(my_tensor + 1, name=\"Mark\")\n",
    "\n",
    "# These are elementwise-unequal, despite having the same name\n",
    "print(a == b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable names are preserved when saving and loading models. By default, variables in models will acquire unique variable names automatically, so you don't need to assign them yourself unless you want to.\n",
    "\n",
    "Although variables are important for differentiation, some variables will not need to be differentiated. You can turn off gradients for a variable by setting trainable to false at creation. An example of a variable that would not need gradients is a training step counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_counter = tf.Variable(1, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placing variables and tensors\n",
    "\n",
    "For better performance, TensorFlow will attempt to place tensors and variables on the fastest device compatible with its `dtype`.  This means most variables are placed on a GPU if one is available.\n",
    "\n",
    "However, we can override this.  In this snippet, we can place a float tensor and a variable on the CPU, even if a GPU is available.  By turning on device placement logging, we can see where the variable is placed. \n",
    "\n",
    "Note: Although manual placement works, using [distribution strategies](distributed_training) can be a more convenient and scalable way to optimize your computation.\n",
    "\n",
    "If you run this notebook on different backends with and without a GPU you will see different logging.  *Note that logging device placement must be turned on at the start of the session.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('CPU:0'):\n",
    "\n",
    "  # Create some tensors\n",
    "  a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "  c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to set the location of a variable or tensor on one device and do the computation on another device.  This will introduce delay, as data needs to be copied between the devices.\n",
    "\n",
    "You might do this, however, if you had multiple GPU workers but only want one copy of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.  4.  9.]\n",
      " [ 4. 10. 18.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('CPU:0'):\n",
    "  a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "  b = tf.Variable([[1.0, 2.0, 3.0]])\n",
    "\n",
    "with tf.device('GPU:0'):\n",
    "  # Element-wise multiply\n",
    "  k = a * b\n",
    "\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Because `tf.config.set_soft_device_placement` is turned on by default, even if you run this code on a device without a GPU, it will still run and the multiplication step happen on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
